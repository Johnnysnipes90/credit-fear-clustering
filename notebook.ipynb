{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"C:/Users/USER/Documents/my_DS_projects/UNSUPERVISED/credit-fear-clustering\")\n",
    "print(\"Current Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv(\"SCFP2019.csv\")\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter households that are credit fearful\n",
    "mask = df['TURNFEAR'] == 1\n",
    "df_fear = df[mask]\n",
    "print(\"Credit-Fearful Households Shape:\", df_fear.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agecl_dict = {\n",
    "    1: \"Under 35\",\n",
    "    2: \"35-44\",\n",
    "    3: \"45-54\",\n",
    "    4: \"55-64\",\n",
    "    5: \"65-74\",\n",
    "    6: \"75 or Older\",\n",
    "}\n",
    "\n",
    "age_cl = df_fear['AGECL'].replace(agecl_dict)\n",
    "print(\"age_cl type:\", type(age_cl))\n",
    "print(\"age_cl shape:\", age_cl.shape)\n",
    "age_cl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age group mapping\n",
    "agecl_dict = {\n",
    "    1: \"Under 35\", 2: \"35-44\", 3: \"45-54\",\n",
    "    4: \"55-64\", 5: \"65-74\", 6: \"75 or Older\"\n",
    "}\n",
    "\n",
    "df_fear = df_fear.copy()  # Ensure df_fear is a separate DataFrame\n",
    "df_fear['AGE_GROUP'] = df_fear['AGECL'].replace(agecl_dict)\n",
    "\n",
    "print(\"df_fear type:\", type(df_fear))\n",
    "print(\"df_fear shape:\", df_fear.shape)\n",
    "df_fear.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_cl_value_counts = age_cl.value_counts()\n",
    "\n",
    "# Bar plot of `age_cl_value_counts`\n",
    "age_cl_value_counts.plot(\n",
    "    kind = 'bar',\n",
    "    xlabel = 'Age Group',\n",
    "    ylabel = 'Frequency [count]',\n",
    "    title = 'Credict Fearful: Age Groups'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age group distribution\n",
    "plt.figure(figsize=(8,5))\n",
    "df_fear['AGE_GROUP'].value_counts().plot(kind='bar', color='skyblue')\n",
    "plt.xlabel(\"Age Group\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Credit Fearful: Age Groups\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our chart is telling us that many of the people who fear being denied credit are younger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of \"AGE\"\n",
    "df_fear['AGE'].hist(bins = 10)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency counts')\n",
    "plt.title('Credit Fearful: Age Distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of \"AGE\"\n",
    "df_fear['AGE'].hist(bins = 10)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency counts')\n",
    "plt.title('Credit Fearful: Age Distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like younger people are still more concerned about being able to secure a loan than older people, but the people who are most concerned seem to be between 30 and 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RACE Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_dict = {\n",
    "    1: \"White/Non-Hispanic\",\n",
    "    2: \"Black/African-American\",\n",
    "    3: \"Hispanic\",\n",
    "    5: \"Other\",\n",
    "}\n",
    "race = df_fear['RACE'].replace(race_dict)\n",
    "race_value_counts = race.value_counts(normalize = True)\n",
    "# Create bar chart of race_value_counts\n",
    "race_value_counts.plot(kind = 'barh')\n",
    "plt.xlim((0, 1))\n",
    "plt.xlabel(\"Frequency (%)\")\n",
    "plt.ylabel(\"Race\")\n",
    "plt.title(\"Credit Fearful: Racial Groups\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race = df['RACE'].replace(race_dict)\n",
    "race_value_counts = race.value_counts(normalize = True)\n",
    "# Create bar chart of race_value_counts\n",
    "race_value_counts.plot(kind = 'barh')\n",
    "plt.xlim((0, 1))\n",
    "plt.xlabel(\"Frequency (%)\")\n",
    "plt.ylabel(\"Race\")\n",
    "plt.title(\"SCF Respondents: Racial Groups\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_dict = {1: \"White/Non-Hispanic\", 2: \"Black/African-American\", 3: \"Hispanic\", 5: \"Other\"}\n",
    "df_fear['RACE_GROUP'] = df_fear['RACE'].replace(race_dict)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "df_fear['RACE_GROUP'].value_counts(normalize=True).plot(kind='bar', color='lightcoral')\n",
    "plt.xlabel(\"Race\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.title(\"Credit Fearful: Racial Groups\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreate the  bar chart i just made, but this time use the entire dataset df instead of the subset df_fear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race = df['RACE'].replace(race_dict)\n",
    "race_value_counts = race.value_counts(normalize = True)\n",
    "# Create bar chart of race_value_counts\n",
    "race_value_counts.plot(kind = 'barh')\n",
    "plt.xlim((0, 1))\n",
    "plt.xlabel(\"Frequency (%)\")\n",
    "plt.ylabel(\"Race\")\n",
    "plt.title(\"SCF Respondents: Racial Groups\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inccat_dict = {\n",
    "    1: \"0-20\",\n",
    "    2: \"21-39.9\",\n",
    "    3: \"40-59.9\",\n",
    "    4: \"60-79.9\",\n",
    "    5: \"80-89.9\",\n",
    "    6: \"90-100\",\n",
    "}\n",
    "\n",
    "df_inccat = (\n",
    "    df['INCCAT']\n",
    "    .replace(inccat_dict)\n",
    "    .groupby(df['TURNFEAR'])\n",
    "    .value_counts(normalize = True)\n",
    "    .rename('frequency')\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"df_inccat type:\", type(df_inccat))\n",
    "print(\"df_inccat shape:\", df_inccat.shape)\n",
    "df_inccat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inccat_dict = {\n",
    "    1: \"0-20\",\n",
    "    2: \"21-39.9\",\n",
    "    3: \"40-59.9\",\n",
    "    4: \"60-79.9\",\n",
    "    5: \"80-89.9\",\n",
    "    6: \"90-100\",\n",
    "}\n",
    "\n",
    "df_inccat = (\n",
    "    df['INCCAT']\n",
    "    .replace(inccat_dict)\n",
    "    .groupby(df['TURNFEAR'])\n",
    "    .value_counts(normalize = True)\n",
    "    .rename('frequency')\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"df_inccat type:\", type(df_inccat))\n",
    "print(\"df_inccat shape:\", df_inccat.shape)\n",
    "df_inccat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar chart of `df_inccat`\n",
    "sns.barplot(\n",
    "    x = 'INCCAT',\n",
    "    y = 'frequency',\n",
    "    hue = 'TURNFEAR',\n",
    "    data = df_inccat,\n",
    "    order = inccat_dict.values()\n",
    "    \n",
    ")\n",
    "plt.xlabel(\"Income Category\")\n",
    "plt.ylabel(\"Frequency (%)\")\n",
    "plt.title(\"Income Distribution: Credit Fearful vs. Non-fearful\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the income categories across the fearful and non-fearful groups, we can see that credit fearful households are much more common in the lower income categories. In other words, the credit fearful have lower incomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inccat_dict = {\n",
    "    1: \"0-20\", 2: \"21-39.9\", 3: \"40-59.9\", 4: \"60-79.9\", 5: \"80-89.9\", 6: \"90-100\"\n",
    "}\n",
    "df_fear['INCOME_CATEGORY'] = df_fear['INCCAT'].replace(inccat_dict)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(x='INCOME_CATEGORY', data=df_fear, order=inccat_dict.values(), palette='coolwarm')\n",
    "plt.xlabel(\"Income Category\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Income Distribution: Credit Fearful Households\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_house_corr = df['ASSET'].corr(df['HOUSES'])\n",
    "print(\"SCF: Asset Houses Correlation:\", asset_house_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a moderate positive correlation, which we would probably expect, right? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_house_corr = df_fear['ASSET'].corr(df_fear['HOUSES'])\n",
    "print(\"Credit Fearful: Asset Houses Correlation:\", asset_house_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! They're different! It's still only a moderate positive correlation, but the relationship between the total value of assets and the value of the primary residence is stronger for our TURNFEAR group than it is for the population as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.scatterplot(x=df_fear['DEBT'], y=df_fear['ASSET'], alpha=0.5, color='purple')\n",
    "plt.xlabel(\"Debt\")\n",
    "plt.ylabel(\"Assets\")\n",
    "plt.title(\"Credit Fearful: Debt vs Assets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Education Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(x='EDUC', data=df_fear, palette='magma')\n",
    "plt.xlabel(\"Education Level\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Education Level: Credit Fearful Households\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_educ = (\n",
    "    df['EDUC']\n",
    "    .groupby(df['TURNFEAR'])\n",
    "    .value_counts(normalize = True)\n",
    "    .rename('frequency')\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    ")\n",
    "print(\"df_educ type:\", type(df_educ))\n",
    "print(\"df_educ shape:\", df_educ.shape)\n",
    "df_educ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar chart of `df_educ`\n",
    "sns.barplot(\n",
    "    x = 'EDUC',\n",
    "    y = 'frequency',\n",
    "    hue = 'TURNFEAR',\n",
    "    data = df_educ\n",
    ")\n",
    "plt.xlabel(\"Education Level\")\n",
    "plt.ylabel(\"Frequency (%)\")\n",
    "plt.title(\"Educational Attainment: Credit Fearful vs. Non-fearful\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot, we can see that a much higher proportion of credit-fearful respondents have only a high school diploma, while university degrees are more common among the non-credit fearful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debt Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot of ASSET vs DEBT, df\n",
    "df.plot.scatter(x = 'DEBT', y = 'ASSET');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot of ASSET vs DEBT, df\n",
    "df_fear.plot.scatter(x = 'DEBT', y = 'ASSET');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot of HOUSES vs DEBT, df\n",
    "df.plot.scatter(x = 'DEBT', y = 'HOUSES');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for fearful house hold\n",
    "selected_cols = [\"ASSET\", \"HOUSES\", \"INCOME\", \"DEBT\", \"EDUC\"]\n",
    "corr_matrix = df_fear[selected_cols].corr()\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix: Financial Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlatio analysis for non credit fearful\n",
    "\n",
    "selected_cols = [\"ASSET\", \"HOUSES\", \"INCOME\", \"DEBT\", \"EDUC\"]\n",
    "corr_matrix = df[selected_cols].corr()\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix: Financial Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa! There are some pretty important differences here! The relationship between \"DEBT\" and \"HOUSES\" is positive for both datasets, but while the coefficient for df is fairly weak at 0.26, the same number for df_fear is 0.96."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import trimmed_var\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wranglefilepath(filepath):\n",
    "\n",
    "    \"\"\"Read SCF data file into ``DataFrame``.\n",
    "\n",
    "    Returns only credit fearful households whose net worth is less than $2 million.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Location of CSV file.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(filepath)\n",
    "    # Create Mask\n",
    "    mask = (df['TURNFEAR'] == 1) & (df['NETWORTH'] < 2e6)\n",
    "    # Subset DataFrame\n",
    "    df = df[mask]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wranglefilepath(\"SCFP2019.csv\")\n",
    "\n",
    "print(\"df type:\", type(df))\n",
    "print(\"df shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the very high number of features we have in our dataset the features to use for the cluster analysis might be a problem. One way to choose the best features for clustering is to determine which numerical features have the largest variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate variance, get 10 largest features\n",
    "top_ten_var = df.var().sort_values().tail(10)\n",
    "\n",
    "print(\"top_ten_var type:\", type(top_ten_var))\n",
    "print(\"top_ten_var shape:\", top_ten_var.shape)\n",
    "top_ten_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create horizontal bar chart of `top_ten_var`\n",
    "fig = px.bar(\n",
    "    x = top_ten_var,\n",
    "    y = top_ten_var.index,\n",
    "    title = 'SCF: High Variance Features'\n",
    "    \n",
    ")\n",
    "fig.update_layout(xaxis_title = 'Variance', yaxis_title = 'Features')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot of `NHNFIN`\n",
    "fig = px.box(\n",
    "    data_frame = df,\n",
    "    x = 'NHNFIN',\n",
    "    title = 'Distribution of Non-home, Non-Financial Assets'\n",
    ")\n",
    "fig.update_layout(xaxis_title = 'Value ($)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is massively right-skewed because of the huge outliers on the right side of the distribution. Even though we already excluded households with a high net worth with our wrangle function, the variance is still being distorted by some extreme outliers.\n",
    "The best way to deal with this is to look at the trimmed variance, where we remove extreme values before calculating variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate trimmed variance\n",
    "top_ten_trim_var = df.apply(trimmed_var, limits = (0.1, 0.1)).sort_values().tail(10)\n",
    "\n",
    "print(\"top_ten_trim_var type:\", type(top_ten_trim_var))\n",
    "print(\"top_ten_trim_var shape:\", top_ten_trim_var.shape)\n",
    "top_ten_trim_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create horizontal bar chart of `top_ten_trim_var`\n",
    "fig = px.bar(\n",
    "    x = top_ten_trim_var,\n",
    "    y = top_ten_trim_var.index,\n",
    "    title = 'SCF: High Variance Features'\n",
    ")\n",
    "fig.update_layout(xaxis_title = 'Trimmed Variance', yaxis_title = 'Feature')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are three things to notice in this plot. First, the variances have decreased a lot. In our previous chart, the x-axis went up to $80 billion; this one goes up to $12 billion. Second, the top 10 features have changed a bit. All the features relating to business ownership (\"...BUS\") are gone. Finally, we can see that there are big differences in variance from feature to feature. For example, the variance for \"WAGEINC\" is around than $500 million, while the variance for \"ASSET\" is nearly $12 billion. In other words, these features have completely different scales. This is something that we'll need to address before we can make good clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a list of high variance with their variance name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_var_cols = top_ten_trim_var.tail(5).index.to_list()\n",
    "\n",
    "print(\"high_var_cols type:\", type(high_var_cols))\n",
    "print(\"high_var_cols len:\", len(top_ten_trim_var))\n",
    "high_var_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[high_var_cols]\n",
    "\n",
    "print(\"X type:\", type(X))\n",
    "print(\"X shape:\", X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model\n",
    "\n",
    "## Iterate\n",
    "\n",
    "During our EDA, we saw that we had a scale issue among our features. That issue can make it harder to cluster the data, so we'll need to fix that to help our analysis along. One strategy we can use is standardization, a statistical method for putting all the variables in a dataset on the same scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = range(2,13)\n",
    "inertia_errors = []\n",
    "silhouette_scores = []\n",
    "\n",
    "# Add `for` loop to train model and calculate inertia, silhouette score.\n",
    "for k in n_clusters:\n",
    "    # Build Model\n",
    "    model = make_pipeline(StandardScaler(), KMeans(n_clusters = k, n_init = 10, random_state = 42))\n",
    "    # Train Model\n",
    "    model.fit(X)\n",
    "    # Cal Inertia\n",
    "    inertia_errors.append(model.named_steps['kmeans'].inertia_)\n",
    "    # Cal Silhouette score\n",
    "    silhouette_scores.append(\n",
    "        silhouette_score(X, model.named_steps['kmeans'].labels_)\n",
    "    )\n",
    "\n",
    "print(\"inertia_errors type:\", type(inertia_errors))\n",
    "print(\"inertia_errors len:\", len(inertia_errors))\n",
    "print(\"Inertia:\", inertia_errors)\n",
    "print()\n",
    "print(\"silhouette_scores type:\", type(silhouette_scores))\n",
    "print(\"silhouette_scores len:\", len(silhouette_scores))\n",
    "print(\"Silhouette Scores:\", silhouette_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining number clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create line plot of `inertia_errors` vs `n_clusters`\n",
    "fig = px.line(\n",
    "    x = n_clusters, y = inertia_errors, title = 'K-Means Model: Inertia vs Number of CLusters'\n",
    ")\n",
    "fig.update_layout(xaxis_title = 'Number of CLusters (k)', yaxis_title = 'Inertial')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the line starts to flatten out around 4 or 5 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line plot of `silhouette_scores` vs `n_clusters`\n",
    "fig = px.line(\n",
    "    x = n_clusters, y = silhouette_scores, title = 'K-Means Model: Silhouette Score vs Number of Clusters'\n",
    ")\n",
    "fig.update_layout(xaxis_title = 'Number of Clusters (k)', yaxis_title = 'Silhouette Score')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one's a little less straightforward, but we can see that the best silhouette scores occur when there are 3 or 4 clusters.\n",
    "Putting the information from this plot together with our inertia plot, it seems like the best setting for n_clusters will be 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import check_is_fitted\n",
    "# Build model\n",
    "final_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    KMeans(n_clusters = 4, n_init = 10, random_state = 42)\n",
    ")\n",
    "\n",
    "# Fit model to data\n",
    "final_model.fit(X)\n",
    "\n",
    "# Assert that model has been fit to data\n",
    "check_is_fitted(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = final_model.named_steps['kmeans'].labels_\n",
    "\n",
    "print(\"labels type:\", type(labels))\n",
    "print(\"labels len:\", len(labels))\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = X.groupby(labels).mean()\n",
    "\n",
    "print(\"xgb type:\", type(xgb))\n",
    "print(\"xgb shape:\", xgb.shape)\n",
    "xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side bar chart of `xgb`\n",
    "fig = px.bar(\n",
    "    xgb,\n",
    "    barmode = 'group',\n",
    "    title = 'Mean Household Finances by Cluster'\n",
    ")\n",
    "fig.update_layout(xaxis_title = 'Cluster', yaxis_title = 'value [$]')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that our clusters are based partially on NETWORTH, which means that the households in the 0 cluster have the smallest net worth, and the households in the 2 cluster have the highest. Based on that, there are some interesting things to unpack here.\n",
    "\n",
    "First, take a look at the DEBT variable. You might think that it would scale as net worth increases, but it doesn't. The lowest amount of debt is carried by the households in cluster 2, even though the value of their houses (shown in green) is roughly the same. You can't really tell from this data what's going on, but one possibility might be that the people in cluster 2 have enough money to pay down their debts, but not quite enough money to leverage what they have into additional debts. The people in cluster 3, by contrast, might not need to worry about carrying debt because their net worth is so high.\n",
    "\n",
    "Finally, since we started out this project looking at home values, take a look at the relationship between DEBT and HOUSES. The value of the debt for the people in cluster 0 is higher than the value of their houses, suggesting that most of the debt being carried by those people is tied up in their mortgages â€” if they own a home at all. Contrast that with the other three clusters: the value of everyone else's debt is lower than the value of their homes.\n",
    "\n",
    "So all that's pretty interesting, but it's different from what we did last time, right? At this point in the last lesson, we made a scatter plot. This was a straightforward task because we only worked with two features, so we could plot the data points in two dimensions. But now X has five dimensions! How can we plot this to give stakeholders a sense of our clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Instantiate transformer\n",
    "pca = PCA(n_components = 2, random_state = 42)\n",
    "\n",
    "# Transform `X`\n",
    "X_t = pca.fit_transform(X)\n",
    "\n",
    "# Put `X_t` into DataFrame\n",
    "X_pca = pd.DataFrame(X_t, columns = ['PC1', 'PC2'])\n",
    "\n",
    "print(\"X_pca type:\", type(X_pca))\n",
    "print(\"X_pca shape:\", X_pca.shape)\n",
    "X_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot of `PC2` vs `PC1`\n",
    "fig = px.scatter(\n",
    "    data_frame = X_pca,\n",
    "    x = 'PC1',\n",
    "    y = 'PC2',\n",
    "    color = labels.astype(str),\n",
    "    title = 'PCA Representation of Clusters'\n",
    ")\n",
    "fig.update_layout(xaxis_title = 'PC1', yaxis_title = 'PC2')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Interactive Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dash import Input, Output, dcc, html, Dash\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats.mstats import trimmed_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app type: <class 'dash.dash.Dash'>\n"
     ]
    }
   ],
   "source": [
    "app = Dash(__name__)\n",
    "print(\"app type:\", type(app))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dashboard\n",
    "\n",
    "### App Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.layout = html.Div(\n",
    "    [\n",
    "        # Application title\n",
    "        html.H1('Survey of Consumer Finances '),\n",
    "        \n",
    "        # Bar chart section\n",
    "        html.H2('High Variance Features'),\n",
    "        dcc.Graph(id='bar-chart'),\n",
    "        \n",
    "        # Trimmed/Not Trimmed selection\n",
    "        html.Label('Select Variance Calculation Method:'),\n",
    "        dcc.RadioItems(\n",
    "            options=[\n",
    "                {'label': 'Trimmed', 'value': True},\n",
    "                {'label': 'Not Trimmed', 'value': False}\n",
    "            ],\n",
    "            value=False,\n",
    "            id='trim-button',\n",
    "            inline=True  # Display options inline\n",
    "        ),\n",
    "        \n",
    "        # K-means clustering section\n",
    "        html.H2('K-means Clustering'),\n",
    "        html.H3('Number of Clusters (k)'),\n",
    "        dcc.Slider(min=2, max=12, step=1, value=2, id='k-slider'),\n",
    "        html.Div(id='metrics'),\n",
    "        \n",
    "        # PCA scatter plot\n",
    "        dcc.Graph(id='pca-scatter')\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_high_var_features(trimmed = True, return_feat_names = True):\n",
    "\n",
    "    \"\"\"Returns the five highest-variance features of ``df``.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trimmed : bool, default=True\n",
    "        If ``True``, calculates trimmed variance, removing bottom and top 10%\n",
    "        of observations.\n",
    "\n",
    "    return_feat_names : bool, default=False\n",
    "        If ``True``, returns feature names as a ``list``. If ``False``\n",
    "        returns ``Series``, where index is feature names and values are\n",
    "        variances.\n",
    "    \"\"\"\n",
    "    # Cal variance\n",
    "    if trimmed:\n",
    "        top_five_features = (df.apply(trimmed_var).sort_values().tail(5))\n",
    "    else:\n",
    "        top_five_features = df.var().sort_values().tail(5)\n",
    "    # Extract names\n",
    "    if return_feat_names:\n",
    "        top_five_features = top_five_features.index.tolist()\n",
    "    return top_five_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.callback(\n",
    "    Output('bar-chart', 'figure'),\n",
    "    Input('trim-button', 'value')\n",
    ")\n",
    "def serve_bar_chart(trimmed = True):\n",
    "\n",
    "    \"\"\"Returns a horizontal bar chart of five highest-variance features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trimmed : bool, default=True\n",
    "        If ``True``, calculates trimmed variance, removing bottom and top 10%\n",
    "        of observations.\n",
    "    \"\"\"\n",
    "    # Get features\n",
    "    top_five_features = get_high_var_features(trimmed = trimmed, return_feat_names = False)\n",
    "    # Build bar chart\n",
    "    fig = px.bar(x = top_five_features, y = top_five_features.index, orientation = 'h')\n",
    "    fig.update_layout(xaxis_title = 'Variance', yaxis_title = 'Feature')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Slider and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_metrics(trimmed = True, k = 2, return_metrics = False):\n",
    "\n",
    "    \"\"\"Build ``KMeans`` model based on five highest-variance features in ``df``.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trimmed : bool, default=True\n",
    "        If ``True``, calculates trimmed variance, removing bottom and top 10%\n",
    "        of observations.\n",
    "\n",
    "    k : int, default=2\n",
    "        Number of clusters.\n",
    "\n",
    "    return_metrics : bool, default=False\n",
    "        If ``False`` returns ``KMeans`` model. If ``True`` returns ``dict``\n",
    "        with inertia and silhouette score.\n",
    "\n",
    "    \"\"\"\n",
    "    features = get_high_var_features(trimmed = trimmed, return_feat_names = True)\n",
    "    # Create feature matrix\n",
    "    X = df[features]\n",
    "    # Build model\n",
    "    model = make_pipeline(StandardScaler(), KMeans(n_clusters = k, n_init = 'auto', random_state = 42))\n",
    "    model.fit(X)\n",
    "    if return_metrics:\n",
    "        # cal inertia\n",
    "        i = model.named_steps['kmeans'].inertia_\n",
    "        # Cal silhouette score\n",
    "        ss = silhouette_score(X, model.named_steps['kmeans'].labels_)\n",
    "        # put result into dictionary\n",
    "        metrics = {\n",
    "            'inertia':round(i),\n",
    "            'silhouette': round(ss, 3)\n",
    "        }\n",
    "        # Return dictionary to user\n",
    "        return metrics\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.callback(\n",
    "    Output('metrics', 'children'),\n",
    "    Input('trim-button', 'value'),\n",
    "    Input('k-slider', 'value')\n",
    ")\n",
    "def serve_metrics(trimmed = True, k = 2):\n",
    "\n",
    "    \"\"\"Returns list of ``H3`` elements containing inertia and silhouette score\n",
    "    for ``KMeans`` model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trimmed : bool, default=True\n",
    "        If ``True``, calculates trimmed variance, removing bottom and top 10%\n",
    "        of observations.\n",
    "\n",
    "    k : int, default=2\n",
    "        Number of clusters.\n",
    "    \"\"\"\n",
    "    # Get metrics\n",
    "    metrics = get_model_metrics(trimmed = trimmed, k = k, return_metrics = True)\n",
    "    # Add metrics to HTML elements\n",
    "    text = [\n",
    "        html.H3(f\"Inertia: {metrics['inertia']}\"),\n",
    "        html.H3(f\"Silhouette Score: {metrics['silhouette']}\")\n",
    "    ]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca_labels(trimmed = True, k = 2):\n",
    "\n",
    "    \"\"\"\n",
    "    ``KMeans`` labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trimmed : bool, default=True\n",
    "        If ``True``, calculates trimmed variance, removing bottom and top 10%\n",
    "        of observations.\n",
    "\n",
    "    k : int, default=2\n",
    "        Number of clusters.\n",
    "    \"\"\"\n",
    "    # Create feature matrix\n",
    "    features = get_high_var_features(trimmed = trimmed, return_feat_names = True)\n",
    "    X = df[features]\n",
    "    # Build transformer\n",
    "    transformer = PCA(n_components = 2, random_state = 42)\n",
    "    # Transform data\n",
    "    X_t = transformer.fit_transform(X)\n",
    "    X_pca = pd.DataFrame(X_t, columns = ['PC1', 'PC2'])\n",
    "    # Add labels\n",
    "    model = get_model_metrics(trimmed = trimmed, k = k, return_metrics = False)\n",
    "    X_pca['labels'] = model.named_steps['kmeans'].labels_.astype(str)\n",
    "    X_pca.sort_values('labels', inplace = True)\n",
    "    return X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.callback(\n",
    "    Output('pca-scatter', 'figure'),\n",
    "    Input('trim-button', 'value'),\n",
    "    Input('k-slider', 'value')\n",
    ")\n",
    "def serve_scatter_plot(trimmed = True, k = 2):\n",
    "\n",
    "    \"\"\"Build 2D scatter plot of ``df`` with ``KMeans`` labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trimmed : bool, default=True\n",
    "        If ``True``, calculates trimmed variance, removing bottom and top 10%\n",
    "        of observations.\n",
    "\n",
    "    k : int, default=2\n",
    "        Number of clusters.\n",
    "    \"\"\"\n",
    "    fig = px.scatter(\n",
    "        data_frame = get_pca_labels(trimmed = trimmed, k = k),\n",
    "        x = 'PC1',\n",
    "        y = 'PC2',\n",
    "        color = 'labels',\n",
    "        title = 'PCA Representation of Clusters'\n",
    "    )\n",
    "    fig.update_layout(xaxis_title = 'PC1', yaxis_title = 'PC2')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x270460ef670>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
